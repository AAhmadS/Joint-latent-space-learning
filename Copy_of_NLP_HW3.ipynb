{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP-HW3/blob/main/Copy_of_NLP_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycCs7DfEpX1u"
      },
      "source": [
        "## Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jtkeu_Wx0ai",
        "outputId": "fee805b7-601d-464c-a5a4-5d8755fd7661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/TasnimDataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DCVttnZJ5OG",
        "outputId": "ec8fbadb-a4cc-4d29-f603-dbe7931e0e06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1WC54iNX9iiUwBMeH0jXlgq2JSuERNvwm/TasnimDataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "b966d153-dbf3-4b4b-cb42-bd595b592e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-joisjz51\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-joisjz51\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic-reshaper in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dadmatools in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: bpemb>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.3.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.8.1)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.14.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.5.2)\n",
            "Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.0.post5)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.29.2)\n",
            "Requirement already satisfied: h5py>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.8.0)\n",
            "Requirement already satisfied: Deprecated==1.2.6 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.2.6)\n",
            "Requirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.2.7)\n",
            "Requirement already satisfied: pyconll>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.1.0)\n",
            "Requirement already satisfied: pytorch-transformers>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.2.0)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.5.11)\n",
            "Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.8.10)\n",
            "Requirement already satisfied: supar==1.1.2 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.1.2)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.3.1)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.5.2)\n",
            "Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.6.6)\n",
            "Requirement already satisfied: NERDA in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.0.0)\n",
            "Requirement already satisfied: py7zr>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.20.5)\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2020.1.16)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (from supar==1.1.2->dadmatools) (1.5.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.3->dadmatools) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.3->dadmatools) (2.27.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.3->dadmatools) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.3.1->dadmatools) (3.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.3.1->dadmatools) (4.11.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (1.6.7)\n",
            "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (3.18.0)\n",
            "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (0.15.7)\n",
            "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.0)\n",
            "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.1)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (0.2.3)\n",
            "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (0.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (5.9.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (1.26.146)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.10.31)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (0.0.53)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.1->dadmatools) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.1->dadmatools) (0.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from NERDA->dadmatools) (1.5.3)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.10/dist-packages (from NERDA->dadmatools) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->dadmatools) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->dadmatools) (1.2.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.9.1->dadmatools) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.4.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.146 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (1.29.146)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (0.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NERDA->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NERDA->dadmatools) (2022.7.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza->supar==1.1.2->dadmatools) (2.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanza->supar==1.1.2->dadmatools) (3.20.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->dadmatools) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install transformers\n",
        "! pip install arabic-reshaper\n",
        "! pip install python-bidi\n",
        "# ! pip install hazm\n",
        "! pip install dadmatools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C1hkDT38hSaP",
        "outputId": "e5a312b0-2a6d-4255-84df-e1b16ef80ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.0.1+cu118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fd48399d9bba>:36: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals\n",
        "# from hazm import Normalizer as hNormalizer\n",
        "from dadmatools.models.normalizer import Normalizer as dNormalizer\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import PIL\n",
        "import torch\n",
        "import os\n",
        "import dill\n",
        "import clip\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from pkg_resources import packaging\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from bidi.algorithm import get_display\n",
        "from arabic_reshaper import reshape\n",
        "from torch.cuda.amp import autocast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CLIPModel, CLIPConfig, CLIPVisionModel, CLIPFeatureExtractor\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModel, TFAutoModel, AutoConfig\n",
        "from transformers import BertModel\n",
        "from transformers import TrainingArguments, Trainer, RobertaModel\n",
        "from transformers import default_data_collator\n",
        "from IPython.display import clear_output\n",
        "import seaborn as sns\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "# persian_font = FontProperties(fname='/content/fonts/Vazirmatn-Regular.ttf')\n",
        "# \n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQT2E9PoWKAn"
      },
      "source": [
        "## Defining the ML-Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2quvXE5pX1z"
      },
      "source": [
        "<div style=\"direction:rtl;\">در ادامه قصد داریم با استفاده از Fine-tune کردن مدل <a href=\"https://github.com/openai/CLIP\">CLIP</a> به یک مدل مناسب برای ترکیب متن و تصویر دست پیدا کنیم:</div> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58ycUu6Nd2C"
      },
      "source": [
        "#### Loading CLIP model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85O8r4TepX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">بررسی مدل های قابل استفاده در CLIP:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vOKzv2bCWKAo",
        "outputId": "bd8fa766-043f-4316-9e59-25f3e60933eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ7TtWVKpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">تعیین استفاده از GPU درصورت وجود:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l79DaXF9WKAo",
        "outputId": "e02c1659-3482-4c8b-9807-7e895e5207cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTwyhMwnpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش مدل CLIP(ViT-L/14) لود شده است و تمامی متغیرهای آن به حالت float32 در آمده است (چون بعضی از متغییر های متبنی بر float16 بودند و به دلیل این که در ادامه قسمت مدل زبانی این مدل تغییر داده شده است، امکان همخوانی با بقیه بخش ها را نداشت.)</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">همچنین از مدل زبان فارسی BERT <a href=\"https://github.com/hooshvare/parsbert\">ParsBERT</a> نیز استفاده شده است.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7xcdNKjEZ7eN",
        "outputId": "ad290599-c3db-4b0a-ef12-d0ad5919dee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased-clf-persiannews were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "model = model.float()\n",
        "parsbert_path = \"HooshvareLab/bert-fa-base-uncased-clf-persiannews\"\n",
        "config = AutoConfig.from_pretrained(parsbert_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(parsbert_path)\n",
        "parsbert = BertModel.from_pretrained(parsbert_path).float().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ete0nQRLMl1J"
      },
      "source": [
        "#### Replaceing the text-encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNjeBzfQpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش در تابع <code>my_encode_text</code> تعریف شده است که از مدل ParsBERT برای بخش زبانی مدل استفاده شود و همچنین در تابع <code>my_tokenizer</code> تعریف شده است که توکنایزر اولیه، متن را به چه شکل توکن کند که این توکنایزر نیز توکنایزر ParsBERT می باشد که به خوبی زبان فارسی را تشخیص می دهد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">در نهایت تابع کدکننده متن را به عنوان انکدر مدل اصلی جایگزین می کنیم و همچنین به عنوان معماری ترنسفورمر نیز مدر ParsBERT را جایگزین می کنیم.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2FK5sqMaUUV"
      },
      "outputs": [],
      "source": [
        "def my_encode_text(text):\n",
        "    return parsbert(text).pooler_output\n",
        "\n",
        "def my_tokenizer(texts):\n",
        "    out_encode = []\n",
        "    for t in texts:\n",
        "        out_encode.append(tokenizer.encode_plus(\n",
        "                t,\n",
        "                max_length=10,\n",
        "                truncation=True,\n",
        "                add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                return_token_type_ids=True,\n",
        "                return_attention_mask=True,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt',  # Return PyTorch tensors\n",
        "            )['input_ids'].numpy().ravel())\n",
        "    return torch.from_numpy(np.array(out_encode))\n",
        "\n",
        "model.encode_text = my_encode_text\n",
        "model.transformer = parsbert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnlqXa1SWKAo"
      },
      "source": [
        "## Prepareing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaicwL5UpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش هدف تقسیم داده ها به دسته آموزش، آزمون و اعتبار می باشد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">ابتدا کلاس <code>CLIPDataset</code> برای این منظور تعریف شده است که وظیفه این را دارد که متن ها را توکنایز کرده و همچنین پیش‌پردازش لازم را روی تصاویر بزند و در نهایت در خروجی یک تاپل از تصویر و متن مربوطه به آن ارائه کند. سپس با استفاده از تابع <code>train_test_split</code> داده های آموزش و تست و اعتبار از هم جدا شده اند و به این کلاس که گفته شده ارسال شده و در نهایت یک <code>DataLoader</code> براساس هر دسته تعریف شده است که متناسب با اندازه بچ خروجی می هد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">هایپرپارامترهای مهم:</div>\n",
        "<ul style=\"direction:rtl;\">\n",
        "    <li>اندازه دسته تست: ۱۵٪ کل داده</li>\n",
        "    <li>اندازه دسته اعتبار: ۱۵٪ داده یادگیری</li>\n",
        "    <li>اندازه بچ: ۶۴</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f=open(\"news.json\")\n",
        "news=json.load(f)"
      ],
      "metadata": {
        "id": "rLmbX06qUp-A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = []\n",
        "objs = []\n",
        "for i in range(len(images_list_final)):\n",
        "  x = images_list_final[i]\n",
        "  try:\n",
        "    open(x,\"rb\")\n",
        "  except(FileNotFoundError, IOError):\n",
        "    indices.append(i)\n",
        "    objs.append(x)"
      ],
      "metadata": {
        "id": "i6acHh4S8vqg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_description_list_final=[]\n",
        "text_title_list_final=[]\n",
        "images_list_final=[]\n",
        "for item in news:\n",
        "  for image in item.get(\"images\"):\n",
        "    if image==\"1400070516181726323700473.jpg\":\n",
        "      continue\n",
        "    if \"images/\"+image in objs:\n",
        "      continue\n",
        "    text_description_list_final.append(item.get(\"description\"))\n",
        "    text_title_list_final.append(item.get(\"title\"))\n",
        "    images_list_final.append(\"images/\"+image)"
      ],
      "metadata": {
        "id": "23HA_SsFYUgc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# images = pd.DataFrame(preprocess(Image.open(images_list_final[0])).reshape(1,-1))\n",
        "# for i in range(len(images_list_final)):\n",
        "#   img_add = images_list_final[i]\n",
        "#   try:\n",
        "#       images = images.append(pd.DataFrame(preprocess(Image.open(img_add)).reshape(1,-1)),ignore_index=True)\n",
        "#       if(i%1024 == 1023):\n",
        "#         with open(f\"images_pd_{i+1}.txt\",\"wb\") as fle:\n",
        "#           pickle.dump(images.drop(0,axis=0),fle)\n",
        "#         print(f\"dumped at {i}\")\n",
        "#         images = pd.DataFrame(preprocess(Image.open(images_list_final[0])).reshape(1,-1))\n",
        "\n",
        "#   except (IOError, OSError, Image.DecompressionBombError) :\n",
        "#     print(f\"{img_add},  {i}\")\n",
        "\n",
        "# with open(f\"images_pd_{i+1}.txt\",\"wb\") as fle:\n",
        "#     pickle.dump(images.drop(0,axis=0),fle)"
      ],
      "metadata": {
        "id": "jJQY54SFGC2A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# slipted = [len(w.split(\" \")) for w in text_title_list_final]\n",
        "# plt.hist(slipted)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "AoaYvOmD2PtO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images_list_final[1]"
      ],
      "metadata": {
        "id": "11G2x5HF2VC0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ITQnDH2JWKAo",
        "outputId": "db2ada01-7c67-4d82-a265-0c2545f263df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train]\n",
            "> item[0].shape:  torch.Size([64, 3, 336, 336])\n",
            "> item[1].shape:  torch.Size([64, 10])\n",
            "[test]\n",
            "> item[0].shape:  torch.Size([64, 3, 336, 336])\n",
            "> item[1].shape:  torch.Size([64, 10])\n",
            "[val]\n",
            "> item[0].shape:  torch.Size([64, 3, 336, 336])\n",
            "> item[1].shape:  torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "TEST_SIZE = 0.15\n",
        "VALIDATION_SIZE = 0.15\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, list_image_path, list_txt):\n",
        "        self.image_path = list_image_path\n",
        "        self.text = my_tokenizer(list_txt).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = preprocess(Image.open(self.image_path[idx]))\n",
        "        text = self.text[idx]\n",
        "        return image, text\n",
        "\n",
        "# class CLIPDataset(Dataset):\n",
        "#     def __init__(self, images_path, list_txt):\n",
        "#         self.images_path = images_path\n",
        "#         # self.images = list_images.to(device)\n",
        "#         self.text = my_tokenizer(list_txt).to(device)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.text)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         image = images[idx]\n",
        "#         text = self.text[idx]\n",
        "#         return image, text\n",
        "\n",
        "all_df = pd.DataFrame({'text': text_title_list_final, 'image_path': images_list_final})\n",
        "\n",
        "train_df, test_df = train_test_split(all_df, test_size=TEST_SIZE, shuffle=True, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=VALIDATION_SIZE, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = CLIPDataset(train_df['image_path'].tolist(), train_df['text'].tolist())\n",
        "test_dataset = CLIPDataset(test_df['image_path'].tolist(), test_df['text'].tolist())\n",
        "val_dataset = CLIPDataset(val_df['image_path'].tolist(), val_df['text'].tolist())\n",
        "\n",
        "dataloader = {}\n",
        "dataloader['train'] = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "dataloader['test'] = DataLoader(test_dataset, batch_size = BATCH_SIZE)\n",
        "dataloader['val'] = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "# Show shape of data\n",
        "for phase in ['train', 'test', 'val']:\n",
        "    for item in dataloader[phase]:\n",
        "        print(f'[{phase}]')\n",
        "        print(f'> item[0].shape: ', item[0].shape)\n",
        "        print(f'> item[1].shape: ', item[1].shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28M9T4XYWKAo"
      },
      "source": [
        "## Fine-tune model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCqJ4fMpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش قصد داریم تا مدل ساخته شده را با استفاده از دیتاست مان Fine-tune کنیم.</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOBbl_UHpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">ابتدا لازم است تا یک سری از هایپرپارامترها را در این بخش تعیین کنیم:</div>\n",
        "</br>\n",
        "\n",
        "| Hyper-paremeter                                         | Value                                               |\n",
        "| ------------------------------------------------------- | --------------------------------------------------- |\n",
        "| Number of Epochs [EPOCH]                                |   10                                                |\n",
        "| Learning-Rate [LR]                                      |   1e-7                                              |\n",
        "| Eps for Adam-optimizer [EPS]                            |   1e-9                                              |\n",
        "| Weight decay of Adam-optimizer [WEIGHT_DECAY]           |   0.1                                               |\n",
        "| Maximum of learning-rate value for scheduler [MAX_LR]   |   1e-2                                              |\n",
        "| Optimizer                                               |   Adam                                              |\n",
        "| Learning-rate scheduler [scheduler]                     |   OneCycleLR                                        |\n",
        "| Image and text prediction loss                          |   CrossEntropyLoss                                  |\n",
        "| Number of freeze layers                                 |   20 layer of each module (visual and transformer)  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BE-R41RezF8H"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/drive/MyDrive/clip_trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "32H5ab1nl_QZ"
      },
      "outputs": [],
      "source": [
        "EPOCH = 10\n",
        "LR = 1e-3\n",
        "EPS = 1e-9\n",
        "WEIGHT_DECAY = 0.1\n",
        "MAX_LR = 5e-3\n",
        "MIN_LR = 1e-7\n",
        "BASE_MODEL_PATH = '/content/drive/MyDrive/TasnimDataset/models/clip_trained_model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9-La7qSMVWIZ"
      },
      "outputs": [],
      "source": [
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                             lr=LR,\n",
        "                             betas=(0.9,0.98),\n",
        "                             eps=EPS,\n",
        "                             weight_decay=WEIGHT_DECAY) \n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
        "                                                max_lr=MAX_LR, \n",
        "                                                steps_per_epoch=len(dataloader['train']), \n",
        "                                                epochs=EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gkQhih_ebp1"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش مدل زبانی و مدل تصویر ۲۰ لایه آخر به حالت قابل آموزش هستند و بقیه مدل فریز شده است:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B1HXx-XHpX12"
      },
      "outputs": [],
      "source": [
        "layer_num = [0, 0]\n",
        "freeze_layer_thr = 30\n",
        "freeze_layer_txt_thr = 20\n",
        "\n",
        "for param in model.transformer.parameters():\n",
        "    layer_num[0]+=1\n",
        "for param in model.visual.parameters():\n",
        "    layer_num[1]+=1\n",
        "\n",
        "for i, param in enumerate(model.transformer.parameters()):\n",
        "    if freeze_layer_txt_thr >= layer_num[0] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "for i, param in enumerate(model.visual.parameters()):\n",
        "    if freeze_layer_thr >= layer_num[1] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZF7V8dlpX12"
      },
      "source": [
        "<div style=\"direction:rtl;\">فرآیند آموزش به این شکل است که دارای دو فاز آموزش و اعتبار می باشد، در فاز آموزش گرادیان ها حساب شده و مراحل backpropagation انجام می شود ولی در فاز اعتبار صرفا بررسی می شود که مقدار loss چقدر است برای این داده ها، به نوعی برای مشخص کردن محلی است که مدل به اندازه کافی خوب شده است. که در این آموزش در ایپاک ۴ مدل خوب شده است.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">به منظور محاسبه loss در هنگام یادگیری، مطابق با <a href=\"https://arxiv.org/abs/2103.00020\">مقاله CLIP</a> فرآیند انجام شده است:</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/AAhmadS/NLP-HW3/blob/data/clip_train_loss.png?raw=1\" width=60% /></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cHNzRPPOWKAp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        best_val_loss = 10000000 \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            num = 0\n",
        "\n",
        "            with tqdm(dataloader[phase]) as pbar:\n",
        "              for batch in pbar:\n",
        "                optimizer.zero_grad()\n",
        "                images, texts = batch\n",
        "                images = images.to(device)\n",
        "                texts = texts.to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    logits_per_image, logits_per_text = model(images, texts)\n",
        "                    ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "                    batch_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        batch_loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                total_loss += batch_loss\n",
        "                num += 1\n",
        "              if phase == 'train':\n",
        "                  scheduler.step()\n",
        "\n",
        "              epoch_loss = total_loss / num\n",
        "              pbar.set_description(f'EPOCH:{epoch+1} - Loss: {epoch_loss/(i+1):.4f}')    \n",
        "\n",
        "              \n",
        "              if phase == 'val':\n",
        "                if epoch_loss < best_val_loss:\n",
        "                  best_val_loss = epoch_loss\n",
        "                  torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep.pt')\n",
        "\n",
        "              print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "              if phase == 'train':\n",
        "                  torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep{epoch}.pt')\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3XQlseyeskc3",
        "outputId": "b36f38c3-a1a7-4312-be79-10539992843a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/467 [00:17<1:08:41,  8.86s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-451e4ab274b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-307d6c366f92>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model=model, optimizer=optimizer, scheduler=scheduler, num_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), f'clip_en_fi_ep{1}.pt')\n",
        "          "
      ],
      "metadata": {
        "id": "Bd7-XnKhYbkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding the decoder to the model"
      ],
      "metadata": {
        "id": "oqERFn9YJ6YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q BertGeneration"
      ],
      "metadata": {
        "id": "FmcexjWGMCXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import BertGeneration\n",
        "decoder = BertGenerationDecoder.from_pretrained(\n",
        "    \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
        ")"
      ],
      "metadata": {
        "id": "hRUx1Hm5J8Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##decoding loop\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=5, criterion):\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        best_val_loss = 10000000 \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            num = 0\n",
        "\n",
        "            for batch in tqdm(dataloader[phase]):\n",
        "                optimizer.zero_grad()\n",
        "                images, texts = batch\n",
        "                images = images.to(device)\n",
        "                texts = texts.to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    logits = model(images)\n",
        "                    ground_truth = texts\n",
        "                    batch_loss = criterion(logits, ground_truth)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        batch_loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                total_loss += batch_loss\n",
        "                num += 1\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = total_loss / num\n",
        "            \n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_val_loss:\n",
        "                best_val_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep_dec.pt')\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            if phase == 'train':\n",
        "                torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep_dec{epoch}.pt')\n",
        "            \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"
      ],
      "metadata": {
        "id": "uT98v5aRJ-rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##suitable dataset based on latent vectors extracted from data\n",
        "\n",
        "##todo \n",
        "\n",
        "##give all data latent vectors and save in the drive\n",
        "\n",
        "\n",
        "## create the dataset, it should take vectors and text as input and output"
      ],
      "metadata": {
        "id": "o-YunH6AKBif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## parameters "
      ],
      "metadata": {
        "id": "8sI9zeEvKDfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##run"
      ],
      "metadata": {
        "id": "P4ydHMNYKa3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}