{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP-HW3/blob/main/Copy_of_NLP_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycCs7DfEpX1u"
      },
      "source": [
        "## Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jtkeu_Wx0ai",
        "outputId": "df2251c6-8f87-418c-ce8d-95ff91b585ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/TasnimDataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DCVttnZJ5OG",
        "outputId": "48aa9221-85d9-4500-f125-940b492a0974"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1WC54iNX9iiUwBMeH0jXlgq2JSuERNvwm/TasnimDataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "cbd71d31-8736-423d-9cf7-693062872ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install transformers\n",
        "! pip install arabic-reshaper\n",
        "! pip install python-bidi\n",
        "# ! pip install hazm\n",
        "! pip install dadmatools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "# from hazm import Normalizer as hNormalizer\n",
        "from dadmatools.models.normalizer import Normalizer as dNormalizer\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import PIL\n",
        "import torch\n",
        "import os\n",
        "import dill\n",
        "import clip\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from pkg_resources import packaging\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from bidi.algorithm import get_display\n",
        "from arabic_reshaper import reshape\n",
        "from torch.cuda.amp import autocast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CLIPModel, CLIPConfig, CLIPVisionModel, CLIPFeatureExtractor\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModel, TFAutoModel, AutoConfig\n",
        "from transformers import BertModel\n",
        "from transformers import TrainingArguments, Trainer, RobertaModel\n",
        "from transformers import default_data_collator\n",
        "from IPython.display import clear_output\n",
        "import seaborn as sns\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "# persian_font = FontProperties(fname='/content/fonts/Vazirmatn-Regular.ttf')\n",
        "# \n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQT2E9PoWKAn"
      },
      "source": [
        "## Defining the ML-Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2quvXE5pX1z"
      },
      "source": [
        "<div style=\"direction:rtl;\">در ادامه قصد داریم با استفاده از Fine-tune کردن مدل <a href=\"https://github.com/openai/CLIP\">CLIP</a> به یک مدل مناسب برای ترکیب متن و تصویر دست پیدا کنیم:</div> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58ycUu6Nd2C"
      },
      "source": [
        "#### Loading CLIP model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85O8r4TepX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">بررسی مدل های قابل استفاده در CLIP:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOKzv2bCWKAo"
      },
      "outputs": [],
      "source": [
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ7TtWVKpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">تعیین استفاده از GPU درصورت وجود:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l79DaXF9WKAo"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTwyhMwnpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش مدل CLIP(ViT-L/14) لود شده است و تمامی متغیرهای آن به حالت float32 در آمده است (چون بعضی از متغییر های متبنی بر float16 بودند و به دلیل این که در ادامه قسمت مدل زبانی این مدل تغییر داده شده است، امکان همخوانی با بقیه بخش ها را نداشت.)</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">همچنین از مدل زبان فارسی BERT <a href=\"https://github.com/hooshvare/parsbert\">ParsBERT</a> نیز استفاده شده است.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xcdNKjEZ7eN"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "model = model.float()\n",
        "parsbert_path = \"HooshvareLab/bert-fa-base-uncased-clf-persiannews\"\n",
        "config = AutoConfig.from_pretrained(parsbert_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(parsbert_path)\n",
        "parsbert = BertModel.from_pretrained(parsbert_path).float().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ete0nQRLMl1J"
      },
      "source": [
        "#### Replaceing the text-encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNjeBzfQpX10"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش در تابع <code>my_encode_text</code> تعریف شده است که از مدل ParsBERT برای بخش زبانی مدل استفاده شود و همچنین در تابع <code>my_tokenizer</code> تعریف شده است که توکنایزر اولیه، متن را به چه شکل توکن کند که این توکنایزر نیز توکنایزر ParsBERT می باشد که به خوبی زبان فارسی را تشخیص می دهد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">در نهایت تابع کدکننده متن را به عنوان انکدر مدل اصلی جایگزین می کنیم و همچنین به عنوان معماری ترنسفورمر نیز مدر ParsBERT را جایگزین می کنیم.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2FK5sqMaUUV"
      },
      "outputs": [],
      "source": [
        "def my_encode_text(text):\n",
        "    return parsbert(text).pooler_output\n",
        "\n",
        "def my_tokenizer(texts):\n",
        "    out_encode = []\n",
        "    for t in texts:\n",
        "        out_encode.append(tokenizer.encode_plus(\n",
        "                t,\n",
        "                max_length=10,\n",
        "                truncation=True,\n",
        "                add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                return_token_type_ids=True,\n",
        "                return_attention_mask=True,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt',  # Return PyTorch tensors\n",
        "            )['input_ids'].numpy().ravel())\n",
        "    return torch.from_numpy(np.array(out_encode))\n",
        "\n",
        "model.encode_text = my_encode_text\n",
        "model.transformer = parsbert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnlqXa1SWKAo"
      },
      "source": [
        "## Prepareing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaicwL5UpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش هدف تقسیم داده ها به دسته آموزش، آزمون و اعتبار می باشد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">ابتدا کلاس <code>CLIPDataset</code> برای این منظور تعریف شده است که وظیفه این را دارد که متن ها را توکنایز کرده و همچنین پیش‌پردازش لازم را روی تصاویر بزند و در نهایت در خروجی یک تاپل از تصویر و متن مربوطه به آن ارائه کند. سپس با استفاده از تابع <code>train_test_split</code> داده های آموزش و تست و اعتبار از هم جدا شده اند و به این کلاس که گفته شده ارسال شده و در نهایت یک <code>DataLoader</code> براساس هر دسته تعریف شده است که متناسب با اندازه بچ خروجی می هد.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">هایپرپارامترهای مهم:</div>\n",
        "<ul style=\"direction:rtl;\">\n",
        "    <li>اندازه دسته تست: ۱۵٪ کل داده</li>\n",
        "    <li>اندازه دسته اعتبار: ۱۵٪ داده یادگیری</li>\n",
        "    <li>اندازه بچ: ۶۴</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f=open(\"news.json\")\n",
        "news=json.load(f)"
      ],
      "metadata": {
        "id": "rLmbX06qUp-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_description_list_final=[]\n",
        "text_title_list_final=[]\n",
        "images_list_final=[]\n",
        "for item in news:\n",
        "  for image in item.get(\"images\"):\n",
        "    if image==\"1400070516181726323700473.jpg\":\n",
        "      continue\n",
        "    text_description_list_final.append(item.get(\"description\"))\n",
        "    text_title_list_final.append(item.get(\"title\"))\n",
        "    images_list_final.append(\"images/\"+image)"
      ],
      "metadata": {
        "id": "23HA_SsFYUgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# images = pd.DataFrame(preprocess(Image.open(images_list_final[0])).reshape(1,-1))\n",
        "# for i in range(len(images_list_final)):\n",
        "#   img_add = images_list_final[i]\n",
        "#   try:\n",
        "#       images = images.append(pd.DataFrame(preprocess(Image.open(img_add)).reshape(1,-1)),ignore_index=True)\n",
        "#       if(i%1024 == 1023):\n",
        "#         with open(f\"images_pd_{i+1}.txt\",\"wb\") as fle:\n",
        "#           pickle.dump(images.drop(0,axis=0),fle)\n",
        "#         print(f\"dumped at {i}\")\n",
        "#         images = pd.DataFrame(preprocess(Image.open(images_list_final[0])).reshape(1,-1))\n",
        "\n",
        "#   except (IOError, OSError, Image.DecompressionBombError) :\n",
        "#     print(f\"{img_add},  {i}\")\n",
        "\n",
        "# with open(f\"images_pd_{i+1}.txt\",\"wb\") as fle:\n",
        "#     pickle.dump(images.drop(0,axis=0),fle)"
      ],
      "metadata": {
        "id": "jJQY54SFGC2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# slipted = [len(w.split(\" \")) for w in text_title_list_final]\n",
        "# plt.hist(slipted)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "AoaYvOmD2PtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images_list_final[1]"
      ],
      "metadata": {
        "id": "11G2x5HF2VC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITQnDH2JWKAo"
      },
      "outputs": [],
      "source": [
        "TEST_SIZE = 0.15\n",
        "VALIDATION_SIZE = 0.15\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, list_image_path, list_txt):\n",
        "        self.image_path = list_image_path\n",
        "        self.text = my_tokenizer(list_txt).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = preprocess(Image.open(self.image_path[idx]))\n",
        "        text = self.text[idx]\n",
        "        return image, text\n",
        "\n",
        "# class CLIPDataset(Dataset):\n",
        "#     def __init__(self, images_path, list_txt):\n",
        "#         self.images_path = images_path\n",
        "#         # self.images = list_images.to(device)\n",
        "#         self.text = my_tokenizer(list_txt).to(device)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.text)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         image = images[idx]\n",
        "#         text = self.text[idx]\n",
        "#         return image, text\n",
        "\n",
        "all_df = pd.DataFrame({'text': text_title_list_final, 'image_path': images_list_final})\n",
        "\n",
        "train_df, test_df = train_test_split(all_df, test_size=TEST_SIZE, shuffle=True, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=VALIDATION_SIZE, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = CLIPDataset(train_df['image_path'].tolist(), train_df['text'].tolist())\n",
        "test_dataset = CLIPDataset(test_df['image_path'].tolist(), test_df['text'].tolist())\n",
        "val_dataset = CLIPDataset(val_df['image_path'].tolist(), val_df['text'].tolist())\n",
        "\n",
        "dataloader = {}\n",
        "dataloader['train'] = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "dataloader['test'] = DataLoader(test_dataset, batch_size = BATCH_SIZE)\n",
        "dataloader['val'] = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "# Show shape of data\n",
        "for phase in ['train', 'test', 'val']:\n",
        "    for item in dataloader[phase]:\n",
        "        print(f'[{phase}]')\n",
        "        print(f'> item[0].shape: ', item[0].shape)\n",
        "        print(f'> item[1].shape: ', item[1].shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28M9T4XYWKAo"
      },
      "source": [
        "## Fine-tune model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCqJ4fMpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش قصد داریم تا مدل ساخته شده را با استفاده از دیتاست مان Fine-tune کنیم.</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOBbl_UHpX11"
      },
      "source": [
        "<div style=\"direction:rtl;\">ابتدا لازم است تا یک سری از هایپرپارامترها را در این بخش تعیین کنیم:</div>\n",
        "</br>\n",
        "\n",
        "| Hyper-paremeter                                         | Value                                               |\n",
        "| ------------------------------------------------------- | --------------------------------------------------- |\n",
        "| Number of Epochs [EPOCH]                                |   10                                                |\n",
        "| Learning-Rate [LR]                                      |   1e-7                                              |\n",
        "| Eps for Adam-optimizer [EPS]                            |   1e-9                                              |\n",
        "| Weight decay of Adam-optimizer [WEIGHT_DECAY]           |   0.1                                               |\n",
        "| Maximum of learning-rate value for scheduler [MAX_LR]   |   1e-2                                              |\n",
        "| Optimizer                                               |   Adam                                              |\n",
        "| Learning-rate scheduler [scheduler]                     |   OneCycleLR                                        |\n",
        "| Image and text prediction loss                          |   CrossEntropyLoss                                  |\n",
        "| Number of freeze layers                                 |   20 layer of each module (visual and transformer)  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE-R41RezF8H"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/drive/MyDrive/clip_trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32H5ab1nl_QZ"
      },
      "outputs": [],
      "source": [
        "EPOCH = 10\n",
        "LR = 1e-3\n",
        "EPS = 1e-9\n",
        "WEIGHT_DECAY = 0.1\n",
        "MAX_LR = 5e-3\n",
        "MIN_LR = 1e-7\n",
        "BASE_MODEL_PATH = '/content/drive/MyDrive/TasnimDataset/models/clip_trained_model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-La7qSMVWIZ"
      },
      "outputs": [],
      "source": [
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                             lr=LR,\n",
        "                             betas=(0.9,0.98),\n",
        "                             eps=EPS,\n",
        "                             weight_decay=WEIGHT_DECAY) \n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
        "                                                max_lr=MAX_LR, \n",
        "                                                steps_per_epoch=len(dataloader['train']), \n",
        "                                                epochs=EPOCH)\n",
        "\n",
        "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                        mode='min', \n",
        "                                                        factor=WEIGHT_DECAY, \n",
        "                                                        patience=10, \n",
        "                                                        threshold=0.0001, \n",
        "                                                        threshold_mode='rel', \n",
        "                                                        cooldown=0, min_lr=MIN_LR, \n",
        "                                                        eps=EPS, \n",
        "                                                        verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gkQhih_ebp1"
      },
      "source": [
        "<div style=\"direction:rtl;\">در این بخش مدل زبانی و مدل تصویر ۲۰ لایه آخر به حالت قابل آموزش هستند و بقیه مدل فریز شده است:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1HXx-XHpX12"
      },
      "outputs": [],
      "source": [
        "layer_num = [0, 0]\n",
        "freeze_layer_thr = 30\n",
        "freeze_layer_txt_thr = 20\n",
        "\n",
        "for param in model.transformer.parameters():\n",
        "    layer_num[0]+=1\n",
        "for param in model.visual.parameters():\n",
        "    layer_num[1]+=1\n",
        "\n",
        "for i, param in enumerate(model.transformer.parameters()):\n",
        "    if freeze_layer_txt_thr >= layer_num[0] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "for i, param in enumerate(model.visual.parameters()):\n",
        "    if freeze_layer_thr >= layer_num[1] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZF7V8dlpX12"
      },
      "source": [
        "<div style=\"direction:rtl;\">فرآیند آموزش به این شکل است که دارای دو فاز آموزش و اعتبار می باشد، در فاز آموزش گرادیان ها حساب شده و مراحل backpropagation انجام می شود ولی در فاز اعتبار صرفا بررسی می شود که مقدار loss چقدر است برای این داده ها، به نوعی برای مشخص کردن محلی است که مدل به اندازه کافی خوب شده است. که در این آموزش در ایپاک ۴ مدل خوب شده است.</div>\n",
        "\n",
        "</br>\n",
        "\n",
        "<div style=\"direction:rtl;\">به منظور محاسبه loss در هنگام یادگیری، مطابق با <a href=\"https://arxiv.org/abs/2103.00020\">مقاله CLIP</a> فرآیند انجام شده است:</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/AAhmadS/NLP-HW3/blob/data/clip_train_loss.png?raw=1\" width=60% /></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHNzRPPOWKAp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        best_val_loss = 10000000 \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            num = 0\n",
        "\n",
        "            for batch in tqdm(dataloader[phase]):\n",
        "                optimizer.zero_grad()\n",
        "                images, texts = batch\n",
        "                images = images.to(device)\n",
        "                texts = texts.to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    logits_per_image, logits_per_text = model(images, texts)\n",
        "                    ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "                    batch_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        batch_loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                total_loss += batch_loss\n",
        "                num += 1\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = total_loss / num\n",
        "            \n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_val_loss:\n",
        "                best_val_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep.pt')\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # if phase == 'train':\n",
        "            #     torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep{epoch}.pt')\n",
        "            \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XQlseyeskc3"
      },
      "outputs": [],
      "source": [
        "train_model(model=model, optimizer=optimizer, scheduler=scheduler2, num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCZ36VtTDp20"
      },
      "source": [
        "<div style=\"direction:rtl;\">به دلیل این که دسترسی به کولب قطع شد، ۴ ایپاک اول روی اجرای قبلی بود عکس آن ضمیمه شد.</div>\n",
        "<br>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/AAhmadS/NLP-HW3/blob/data/train_first_4epoch.png?raw=1\" width=60% /></p>"
      ]
    }
  ]
}